{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb893d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asia_\\OneDrive\\Desktop\\SAMRank\\keyphrase_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "from swisscom_ai.research_keyphrase.preprocessing.postagging import PosTaggingCoreNLP\n",
    "from swisscom_ai.research_keyphrase.model.input_representation import InputTextObj\n",
    "from swisscom_ai.research_keyphrase.model.extractor import extract_candidates, extract_verb_candidates\n",
    "\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba4750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'localhost'\n",
    "port = 9000\n",
    "pos_tagger = PosTaggingCoreNLP(host, port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b63c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwords\n",
    "stopwords = []\n",
    "with open('UGIR_stopwords.txt', \"r\") as f:\n",
    "    for line in f:\n",
    "        if line:\n",
    "            stopwords.append(line.replace('\\n', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7372fa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c1310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_sum_token_level(attention_map):\n",
    "    tokens_score = torch.sum(attention_map, dim=0)\n",
    "    return tokens_score\n",
    "\n",
    "\n",
    "def redistribute_global_attention_score(attention_map, tokens_score):\n",
    "    new_attention_map = attention_map * tokens_score.unsqueeze(0)\n",
    "    return new_attention_map\n",
    "\n",
    "\n",
    "def normalize_attention_map(attention_map):\n",
    "    attention_map_sum = attention_map.sum(dim=0, keepdim=True)\n",
    "    attention_map_sum += 1e-10\n",
    "    attention_map_normalized = attention_map / attention_map_sum\n",
    "    return attention_map_normalized\n",
    "\n",
    "\n",
    "def get_row_sum_token_level(attention_map):\n",
    "    tokens_score = torch.sum(attention_map, dim=1)\n",
    "    return tokens_score\n",
    "\n",
    "\n",
    "def aggregate_phrase_scores(index_list, tokens_scores):\n",
    "    total_score = 0.0\n",
    "    \n",
    "    for p_index in index_list:\n",
    "        part_sum = tokens_scores[p_index[0]:p_index[1]].sum()\n",
    "        total_score += part_sum\n",
    "\n",
    "    return total_score\n",
    "\n",
    "# Checkpoint\n",
    "# Get the highest global attention score for each phrase\n",
    "def get_phrase_source(candidates_indices, index_list, attention_map, start_index, end_index, eos_indices):\n",
    "    total_score = torch.zeros(attention_map.shape[1], dtype=torch.float32, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if len(index_list) == 0 or start_index == 0:\n",
    "        return None, 0.0, None, None\n",
    "\n",
    "    # Get global attention scores from all indices in the index_list\n",
    "    for p_index in index_list:\n",
    "        # Skip indices that are out of the range of this sentence\n",
    "        if p_index[0] < start_index or p_index[1] > end_index:\n",
    "            continue\n",
    "\n",
    "        part_sum = attention_map[p_index[0]:p_index[1]].sum(axis=0)\n",
    "        total_score += part_sum\n",
    "    \n",
    "    \n",
    "    # Set scores for itself to 0\n",
    "    for p_index in index_list:\n",
    "        total_score[p_index[0]:p_index[1]] = 0.0\n",
    "        \n",
    "    last_index = index_list[-1][0] \n",
    "\n",
    "    best_phrase = None\n",
    "    best_score = float('-inf')\n",
    "    best_sentence = None\n",
    "    last_source_index = None\n",
    "    for phrase in candidates_indices.keys():\n",
    "        try:\n",
    "            phrase_indices = candidates_indices[phrase]\n",
    "            if len(phrase_indices) == 0:\n",
    "                continue\n",
    "        except KeyError:\n",
    "            continue\n",
    "        phrase_score = aggregate_phrase_scores(phrase_indices, total_score)\n",
    "\n",
    "        \n",
    "\n",
    "        # If the phrase score is better than the best score, update the best phrase and score\n",
    "        if phrase_score > best_score:\n",
    "\n",
    "            # Find end of sentence indices that lower or equal to the last index of the phrase\n",
    "            last_index2 = phrase_indices[-1][1]\n",
    "            for i, idx in enumerate(phrase_indices):\n",
    "                if idx[0] > last_index:\n",
    "                    last_index2 = phrase_indices[i-1][1]\n",
    "                    break\n",
    "\n",
    "            # Get the location of the last index in the phrase\n",
    "            for i, eos_idx in enumerate(eos_indices):\n",
    "                if last_index2 <= eos_idx:\n",
    "                    sentence = i+1\n",
    "                    break\n",
    "                \n",
    "            best_score = phrase_score\n",
    "            best_phrase = phrase\n",
    "            best_sentence = sentence\n",
    "            last_source_index = last_index2\n",
    "    return best_phrase, best_score, best_sentence, last_source_index\n",
    "\n",
    "# Get the highest global attention score for each phrase\n",
    "def get_phrase_relation(candidates_indices, index_list, attention_map, start_index, end_index, source_index):\n",
    "\n",
    "    total_score = torch.zeros(attention_map.shape[1], dtype=torch.float32, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if len(index_list) == 0 or start_index == 0:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "    # Get global attention scores from all indices in the index_list\n",
    "    for p_index in index_list:\n",
    "        # Skip indices that are out of the range of this sentence\n",
    "        if p_index[0] < start_index or p_index[1] > end_index:\n",
    "            continue\n",
    "        \n",
    "        part_sum = attention_map[p_index[0]:p_index[1]].sum(axis=0)\n",
    "        total_score += part_sum\n",
    "    \n",
    "    # Set scores for itself to 0\n",
    "    for p_index in index_list:\n",
    "        total_score[p_index[0]:p_index[1]] = 0.0\n",
    "\n",
    "    \n",
    "    best_phrase = \"\"\n",
    "    best_score = float('-inf')\n",
    "    total_score[:source_index] = 0.0  # Set scores before source index to 0\n",
    "    for phrase in candidates_indices.keys():\n",
    "        try:\n",
    "            phrase_indices = candidates_indices[phrase]\n",
    "            if len(phrase_indices) == 0:\n",
    "                continue\n",
    "        except KeyError:\n",
    "            continue\n",
    "        phrase_score = aggregate_phrase_scores(phrase_indices, total_score)\n",
    "\n",
    "        \n",
    "        # If the phrase score is better than the best score, update the best phrase and score\n",
    "        if phrase_score > best_score:\n",
    "            best_score = phrase_score\n",
    "            best_phrase = phrase\n",
    "\n",
    "    return best_phrase\n",
    "\n",
    "def get_phrase_indices(text_tokens, phrase, prefix):\n",
    "    text_tokens = [t.replace(prefix, '') for t in text_tokens]\n",
    "\n",
    "    phrase = phrase.replace(' ', '')\n",
    "\n",
    "    matched_indices = []\n",
    "    matched_index = []\n",
    "    target = phrase\n",
    "    for i in range(len(text_tokens)):\n",
    "        cur_token = text_tokens[i]\n",
    "        sub_len = min(len(cur_token), len(phrase))\n",
    "        if cur_token[:sub_len].lower() == target[:sub_len]:\n",
    "            matched_index.append(i)\n",
    "            target = target[sub_len:]\n",
    "            if len(target) == 0:\n",
    "                matched_indices.append([matched_index[0], matched_index[-1] + 1])\n",
    "                target = phrase\n",
    "        else:\n",
    "            matched_index = []\n",
    "            target = phrase\n",
    "            if cur_token[:sub_len].lower() == target[:sub_len]:\n",
    "                matched_index.append(i)\n",
    "                target = target[sub_len:]\n",
    "                if len(target) == 0:\n",
    "                    matched_indices.append([matched_index[0], matched_index[-1] + 1])\n",
    "                    target = phrase\n",
    "\n",
    "    return matched_indices\n",
    "\n",
    "def get_verb_indices(text_tokens, phrase, prefix):\n",
    "    # Remove prefix from text tokens\n",
    "    text_tokens = [t.replace(prefix, '') for t in text_tokens]\n",
    "\n",
    "    # Split phrase into tokens (words)\n",
    "    phrase_tokens = phrase.split(' ')\n",
    "\n",
    "    matched_indices = []\n",
    "\n",
    "    # Loop over text tokens to find phrase matches\n",
    "    for i in range(len(text_tokens) - len(phrase_tokens) + 1):\n",
    "        # Slice text tokens of phrase length\n",
    "        window = text_tokens[i:i+len(phrase_tokens)]\n",
    "        # Compare tokens ignoring case\n",
    "        if all(w.lower() == p.lower() for w, p in zip(window, phrase_tokens)):\n",
    "            matched_indices.append([i, i + len(phrase_tokens)])\n",
    "\n",
    "    return matched_indices\n",
    "\n",
    "def remove_repeated_sub_word(candidates_pos_dict):\n",
    "    for phrase in candidates_pos_dict.keys():\n",
    "        split_phrase = phrase.split()\n",
    "        if len(split_phrase) > 1:\n",
    "            for word in split_phrase:\n",
    "                if word in candidates_pos_dict:\n",
    "                    single_word_positions = candidates_pos_dict[word]\n",
    "                    phrase_positions = candidates_pos_dict[phrase]\n",
    "                    single_word_alone_positions = [pos for pos in single_word_positions if not any(\n",
    "                        pos[0] >= phrase_pos[0] and pos[1] <= phrase_pos[1] for phrase_pos in phrase_positions)]\n",
    "                    candidates_pos_dict[word] = single_word_alone_positions\n",
    "\n",
    "    return candidates_pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dea213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_same_len_segments(total_tokens_ids, max_len):\n",
    "    num_of_seg = (len(total_tokens_ids) // max_len) + 1\n",
    "    seg_len = int(len(total_tokens_ids) / num_of_seg)\n",
    "    segments = []\n",
    "    attn_masks = []\n",
    "    for _ in range(num_of_seg):\n",
    "        if len(total_tokens_ids) > seg_len:\n",
    "            segment = total_tokens_ids[:seg_len]\n",
    "            total_tokens_ids = total_tokens_ids[seg_len:]\n",
    "        else:\n",
    "            segment = total_tokens_ids\n",
    "        segments.append(segment)\n",
    "        attn_masks.append([1] * len(segments[-1]))\n",
    "\n",
    "    return segments, attn_masks\n",
    "\n",
    "def read_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line.strip())\n",
    "            data.append(item)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_candidates(core_nlp, text):\n",
    "    tagged = core_nlp.pos_tag_raw_text(text)\n",
    "    text_obj = InputTextObj(tagged, 'en')\n",
    "    candidates = extract_candidates(text_obj)\n",
    "    return candidates\n",
    "\n",
    "# ADDED: Function to get verb candidates from text using CoreNLP\n",
    "def get_verb_candidates(core_nlp, text):\n",
    "    tagged = core_nlp.pos_tag_raw_text(text)\n",
    "    text_obj = InputTextObj(tagged, 'en')\n",
    "    print(text_obj.pos_tagged)\n",
    "    candidates = extract_verb_candidates(text_obj)\n",
    "    return candidates\n",
    "\n",
    "def get_score_full(candidates, references, maxDepth=15):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    reference_set = set(references)\n",
    "    referencelen = len(reference_set)\n",
    "    true_positive = 0\n",
    "    for i in range(maxDepth):\n",
    "        if len(candidates) > i:\n",
    "            kp_pred = candidates[i]\n",
    "            if kp_pred in reference_set:\n",
    "                true_positive += 1\n",
    "            precision.append(true_positive / float(i + 1))\n",
    "            recall.append(true_positive / float(referencelen))\n",
    "        else:\n",
    "            precision.append(true_positive / float(len(candidates)))\n",
    "            recall.append(true_positive / float(referencelen))\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def evaluate(candidates, references):\n",
    "    results = {}\n",
    "    precision_scores, recall_scores, f1_scores = {5: [], 10: [], 15: []}, \\\n",
    "                                                 {5: [], 10: [], 15: []}, \\\n",
    "                                                 {5: [], 10: [], 15: []}\n",
    "    for candidate, reference in zip(candidates, references):\n",
    "        p, r = get_score_full(candidate, reference)\n",
    "        for i in [5, 10, 15]:\n",
    "            precision = p[i - 1]\n",
    "            recall = r[i - 1]\n",
    "            if precision + recall > 0:\n",
    "                f1_scores[i].append((2 * (precision * recall)) / (precision + recall))\n",
    "            else:\n",
    "                f1_scores[i].append(0)\n",
    "            precision_scores[i].append(precision)\n",
    "            recall_scores[i].append(recall)\n",
    "\n",
    "    print(\"########################\\nMetrics\")\n",
    "    for i in precision_scores:\n",
    "        print(\"@{}\".format(i))\n",
    "        print(\"F1:{}\".format(np.mean(f1_scores[i])))\n",
    "        print(\"P:{}\".format(np.mean(precision_scores[i])))\n",
    "        print(\"R:{}\".format(np.mean(recall_scores[i])))\n",
    "\n",
    "        top_n_p = 'precision@' + str(i)\n",
    "        top_n_r = 'recall@' + str(i)\n",
    "        top_n_f1 = 'f1@' + str(i)\n",
    "        results[top_n_p] = np.mean(precision_scores[i])\n",
    "        results[top_n_r] = np.mean(recall_scores[i])\n",
    "        results[top_n_f1] = np.mean(f1_scores[i])\n",
    "    print(\"#########################\")\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a74633e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2', output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9974f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str)\n",
    "parser.add_argument('--plm', type=str)\n",
    "parser.add_argument('--mode', type=str)\n",
    "args = parser.parse_args([\n",
    "    '--dataset', 'SemEval2017',\n",
    "    '--plm', 'GPT2',\n",
    "    '--mode', 'Both'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77412cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == 'Inspec' or args.dataset == 'inpsec':\n",
    "        data_path = 'data/Inspec.jsonl'\n",
    "        doc_type = 'short'\n",
    "elif args.dataset == 'SemEval2010' or args.dataset == 'semeval2010':\n",
    "    data_path = 'data/SemEval2010.jsonl'\n",
    "    doc_type = 'long'\n",
    "elif args.dataset == 'SemEval2017' or args.dataset == 'semeval2017':\n",
    "    data_path = 'data/SemEval2017.jsonl'\n",
    "    doc_type = 'short'\n",
    "elif args.dataset == 'Krapivin' or args.dataset == 'krapivin':\n",
    "    data_path = 'data/krapivin.jsonl'\n",
    "    doc_type = 'long'\n",
    "dataset = read_jsonl(data_path)\n",
    "# rank_short_documents(args, dataset, model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b385337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 117MB/s]                     \n",
      "2025-07-27 22:05:58 INFO: Downloaded file to C:\\Users\\asia_\\stanza_resources\\resources.json\n",
      "2025-07-27 22:05:58 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-07-27 22:05:59 INFO: File exists: C:\\Users\\asia_\\stanza_resources\\en\\default.zip\n",
      "2025-07-27 22:06:02 INFO: Finished downloading models and saved to C:\\Users\\asia_\\stanza_resources\n",
      "2025-07-27 22:06:02 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 65.8MB/s]                    \n",
      "2025-07-27 22:06:02 INFO: Downloaded file to C:\\Users\\asia_\\stanza_resources\\resources.json\n",
      "2025-07-27 22:06:02 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-07-27 22:06:02 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "========================\n",
      "\n",
      "2025-07-27 22:06:02 INFO: Using device: cuda\n",
      "2025-07-27 22:06:02 INFO: Loading: tokenize\n",
      "2025-07-27 22:06:02 INFO: Loading: mwt\n",
      "2025-07-27 22:06:02 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')  # only the first time\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0042f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eos_indices(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sentence.text for sentence in doc.sentences]\n",
    "    sent_token_lens = [len(tokenizer.tokenize(sent)) for sent in sentences]\n",
    "    \n",
    "    # Calculate cumulative sum of tokens to get EOS indices\n",
    "    eos_token_indices = []\n",
    "    cum_sum = 0\n",
    "    for length in sent_token_lens:\n",
    "        cum_sum += length\n",
    "        eos_token_indices.append(cum_sum - 1)\n",
    "    return eos_token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e2dbbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore = ['.', '(', ')', '[', ']', '{', '}', '\"', \"'\", '?', '!', ':', ';', '_', '*', '%', '$', '#', '@']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c0eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flow(args, data, model, tokenizer):\n",
    "    if args.plm == 'BERT':\n",
    "        prefix = '##'\n",
    "    elif args.plm == 'GPT2':\n",
    "        prefix = 'Ġ'\n",
    "\n",
    "    # layer_head_predicted_top15 = defaultdict(list)\n",
    "    sentence_predicted_top = defaultdict(list)\n",
    "    sentence_predicted_source = defaultdict(list)\n",
    "    sentence_predicted_relation = defaultdict(list)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"device: {device}\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # for data in tqdm(dataset):\n",
    "    with torch.no_grad():\n",
    "        new_text = '.'+data['text']\n",
    "        tokenized_text = tokenizer(new_text, return_tensors='pt')\n",
    "\n",
    "        outputs = model(**tokenized_text.to(device))\n",
    "\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "        candidates = get_candidates(pos_tagger, new_text)\n",
    "        candidates = [phrase for phrase in candidates if phrase.split(' ')[0] not in stopwords]\n",
    "        \n",
    "        \n",
    "        # print(candidates)\n",
    "        text_tokens = tokenizer.convert_ids_to_tokens(tokenized_text['input_ids'].squeeze(0))\n",
    "\n",
    "        candidates_indices = {}\n",
    "        for phrase in candidates:\n",
    "            matched_indices = get_phrase_indices(text_tokens, phrase, prefix)\n",
    "            if len(matched_indices) == 0:\n",
    "                continue\n",
    "            candidates_indices[phrase] = matched_indices\n",
    "\n",
    "        candidates_indices = remove_repeated_sub_word(candidates_indices)\n",
    "\n",
    "        \"\"\"   Get Verb Candidates   \"\"\"\n",
    "        verb_candidates = get_verb_candidates(pos_tagger, new_text)\n",
    "        filtered_phrases = []\n",
    "        for phrase in verb_candidates:\n",
    "            words = phrase.split(' ')\n",
    "            # Remove all words in ignore\n",
    "            filtered_words = [w for w in words if w not in ignore]\n",
    "            if filtered_words:  # Only add if not empty\n",
    "                filtered_phrases.append(' '.join(filtered_words))\n",
    "        print(verb_candidates)\n",
    "        verb_candidates_indices = {}\n",
    "        for phrase in filtered_phrases:\n",
    "            matched_indices = get_verb_indices(text_tokens, phrase, prefix)\n",
    "            if len(matched_indices) == 0:\n",
    "                continue\n",
    "            verb_candidates_indices[phrase] = matched_indices\n",
    "\n",
    "        verb_candidates_indices = remove_repeated_sub_word(verb_candidates_indices)\n",
    "        # print(verb_candidates_indices)\n",
    "        \"\"\"   END   \"\"\"   \n",
    "\n",
    "        layer = 10\n",
    "        head = 0\n",
    "        head2 = 0\n",
    "\n",
    "        n_layer_attentions = attentions[layer].squeeze(0)\n",
    "        attention_map = n_layer_attentions[head]\n",
    "        relation_map = n_layer_attentions[head2]\n",
    "        eos_indices = get_eos_indices(new_text)\n",
    "        start_index = 0\n",
    "        for i, index in enumerate(eos_indices):\n",
    "            \n",
    "            # global_attention_scores = get_col_sum_token_level(attention_map)\n",
    "            end_index = index + 1\n",
    "            interested_map = attention_map[:end_index,:end_index]\n",
    "            \n",
    "            global_attention_scores = get_col_sum_token_level(interested_map)\n",
    "\n",
    "            if args.plm == \"BERT\":\n",
    "                global_attention_scores[-1] = 0\n",
    "            elif args.plm == \"GPT2\":\n",
    "                global_attention_scores[0] = 0\n",
    "\n",
    "            # redistributed_attention_map = redistribute_global_attention_score(attention_map,\n",
    "            #                                                                     global_attention_scores)\n",
    "\n",
    "            redistributed_attention_map = normalize_attention_map(interested_map)\n",
    "\n",
    "            proportional_attention_scores = get_row_sum_token_level(redistributed_attention_map)\n",
    "\n",
    "            # if args.mode == 'Both':\n",
    "            #     final_tokens_score = global_attention_scores + proportional_attention_scores\n",
    "            # elif args.mode == 'Global':\n",
    "            #     final_tokens_score = global_attention_scores\n",
    "            # elif args.mode == 'Proportional':\n",
    "            #     final_tokens_score = proportional_attention_scores\n",
    "            final_tokens_score = global_attention_scores + proportional_attention_scores\n",
    "            final_tokens_score[:start_index] = 0.0\n",
    "            # padded_final_tokens_score = F.pad(final_tokens_score, (start_index, 0), mode='constant', value=0)\n",
    "            phrase_score_dict = {}\n",
    "            for phrase in candidates_indices.keys():\n",
    "                try:\n",
    "                    phrase_indices = candidates_indices[phrase]\n",
    "                    if len(phrase_indices) == 0:\n",
    "                        continue\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "                final_phrase_score = aggregate_phrase_scores(phrase_indices, final_tokens_score)\n",
    "                \n",
    "                if len(phrase.split()) == 1:\n",
    "                    final_phrase_score = final_phrase_score / len(phrase_indices)\n",
    "                phrase_score_dict[phrase] = final_phrase_score\n",
    "\n",
    "            # print(phrase_score_dict)\n",
    "            sorted_scores = sorted(phrase_score_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "            # stemmed_sorted_scores = [(\" \".join(stemmer.stem(word) for word in phrase.split()), score) for\n",
    "            #                             phrase, score in sorted_scores]\n",
    "            # print(stemmed_sorted_scores)\n",
    "            set_scores_list = []\n",
    "            for phrase, score in sorted_scores:\n",
    "                # print(phrase)\n",
    "                if phrase not in set_scores_list:\n",
    "                    set_scores_list.append((phrase,score))\n",
    "\n",
    "            pred_stemmed_phrases = set_scores_list[:5]\n",
    "            \n",
    "            source_score_list = []\n",
    "            relation_list = []\n",
    "            \"\"\"   Get Source   \"\"\"\n",
    "\n",
    "            for phrase in pred_stemmed_phrases:\n",
    "                phrase_indices = candidates_indices[phrase[0]]\n",
    "                if len(phrase_indices) == 0:\n",
    "                    continue\n",
    "                \n",
    "                phrase_source, source_score, source_sentence, source_index = get_phrase_source(candidates_indices, phrase_indices, attention_map, start_index, end_index, eos_indices)\n",
    "                source_score_list.append((phrase_source, source_score, source_sentence))\n",
    "\n",
    "                \"\"\"   Get Relation   \"\"\"\n",
    "                \n",
    "                phrase_relation = get_phrase_relation(verb_candidates_indices, phrase_indices, relation_map, start_index, end_index, source_index)\n",
    "                \n",
    "                relation_list.append(phrase_relation)\n",
    "\n",
    "                \"\"\"   END   \"\"\"\n",
    "                \n",
    "            \"\"\"   END   \"\"\"\n",
    "        \n",
    "            # layer_head_predicted_top15[(layer, head)].append(pred_stemmed_phrases)\n",
    "            sentence_predicted_top[i].append(pred_stemmed_phrases)\n",
    "            sentence_predicted_source[i].append(source_score_list)\n",
    "            sentence_predicted_relation[i].append(relation_list)\n",
    "            # for i in pred_stemmed_phrases:\n",
    "            #     if len(sentence_predicted_top[i]) == 0:\n",
    "            #         sentence_predicted_top[i] = []\n",
    "            \n",
    "            start_index = end_index\n",
    "        return sentence_predicted_top, sentence_predicted_source, sentence_predicted_relation    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b831e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flow2(args, data, model, tokenizer):\n",
    "    if args.plm == 'BERT':\n",
    "        prefix = '##'\n",
    "    elif args.plm == 'GPT2':\n",
    "        prefix = 'Ġ'\n",
    "\n",
    "    # layer_head_predicted_top15 = defaultdict(list)\n",
    "    sentence_predicted_top = defaultdict(list)\n",
    "    sentence_predicted_source = defaultdict(list)\n",
    "    sentence_predicted_relation = defaultdict(list)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"device: {device}\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # for data in tqdm(dataset):\n",
    "    with torch.no_grad():\n",
    "        new_text = '.'+data['text']\n",
    "        tokenized_text = tokenizer(new_text, return_tensors='pt')\n",
    "\n",
    "        outputs = model(**tokenized_text.to(device))\n",
    "\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "        candidates = get_candidates(pos_tagger, new_text)\n",
    "        candidates = [phrase for phrase in candidates if phrase.split(' ')[0] not in stopwords]\n",
    "        \n",
    "        \n",
    "        # print(candidates)\n",
    "        text_tokens = tokenizer.convert_ids_to_tokens(tokenized_text['input_ids'].squeeze(0))\n",
    "\n",
    "        candidates_indices = {}\n",
    "        for phrase in candidates:\n",
    "            matched_indices = get_phrase_indices(text_tokens, phrase, prefix)\n",
    "            if len(matched_indices) == 0:\n",
    "                continue\n",
    "            candidates_indices[phrase] = matched_indices\n",
    "\n",
    "        candidates_indices = remove_repeated_sub_word(candidates_indices)\n",
    "\n",
    "        \"\"\"   Get Verb Candidates   \"\"\"\n",
    "        verb_candidates = get_verb_candidates(pos_tagger, new_text)\n",
    "        filtered_phrases = []\n",
    "        for phrase in verb_candidates:\n",
    "            words = phrase.split(' ')\n",
    "            # Remove all words in ignore\n",
    "            filtered_words = [w for w in words if w not in ignore]\n",
    "            if filtered_words:  # Only add if not empty\n",
    "                filtered_phrases.append(' '.join(filtered_words))\n",
    "        print(verb_candidates)\n",
    "        verb_candidates_indices = {}\n",
    "        for phrase in filtered_phrases:\n",
    "            matched_indices = get_verb_indices(text_tokens, phrase, prefix)\n",
    "            if len(matched_indices) == 0:\n",
    "                continue\n",
    "            verb_candidates_indices[phrase] = matched_indices\n",
    "\n",
    "        verb_candidates_indices = remove_repeated_sub_word(verb_candidates_indices)\n",
    "        # print(verb_candidates_indices)\n",
    "        \"\"\"   END   \"\"\"   \n",
    "\n",
    "        layer = 10\n",
    "        head = 0\n",
    "        head2 = 0\n",
    "\n",
    "        n_layer_attentions = attentions[layer].squeeze(0)\n",
    "        attention_map = n_layer_attentions[head]\n",
    "        relation_map = n_layer_attentions[head2]\n",
    "        eos_indices = get_eos_indices(new_text)\n",
    "        start_index = 0\n",
    "        print(f\"eos_indices: {eos_indices}\")\n",
    "        for i, index in enumerate(eos_indices):\n",
    "            \n",
    "            # global_attention_scores = get_col_sum_token_level(attention_map)\n",
    "            end_index = index + 1\n",
    "            interested_map = attention_map[:end_index+1,:end_index+1]\n",
    "            \n",
    "            global_attention_scores = get_col_sum_token_level(interested_map)\n",
    "\n",
    "            if args.plm == \"BERT\":\n",
    "                global_attention_scores[-1] = 0\n",
    "            elif args.plm == \"GPT2\":\n",
    "                global_attention_scores[0] = 0\n",
    "\n",
    "            # redistributed_attention_map = redistribute_global_attention_score(attention_map,\n",
    "            #                                                                     global_attention_scores)\n",
    "\n",
    "            redistributed_attention_map = normalize_attention_map(interested_map)\n",
    "\n",
    "            proportional_attention_scores = get_row_sum_token_level(redistributed_attention_map)\n",
    "\n",
    "            # if args.mode == 'Both':\n",
    "            #     final_tokens_score = global_attention_scores + proportional_attention_scores\n",
    "            # elif args.mode == 'Global':\n",
    "            #     final_tokens_score = global_attention_scores\n",
    "            # elif args.mode == 'Proportional':\n",
    "            #     final_tokens_score = proportional_attention_scores\n",
    "            final_tokens_score = global_attention_scores + proportional_attention_scores\n",
    "            final_tokens_score[:start_index] = 0.0\n",
    "            # padded_final_tokens_score = F.pad(final_tokens_score, (start_index, 0), mode='constant', value=0)\n",
    "            phrase_score_dict = {}\n",
    "            for phrase in candidates_indices.keys():\n",
    "                try:\n",
    "                    phrase_indices = candidates_indices[phrase]\n",
    "                    if len(phrase_indices) == 0:\n",
    "                        continue\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "                final_phrase_score = aggregate_phrase_scores(phrase_indices, final_tokens_score)\n",
    "                \n",
    "                if len(phrase.split()) == 1:\n",
    "                    final_phrase_score = final_phrase_score / len(phrase_indices)\n",
    "                phrase_score_dict[phrase] = final_phrase_score\n",
    "\n",
    "            # print(phrase_score_dict)\n",
    "            sorted_scores = sorted(phrase_score_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "            # stemmed_sorted_scores = [(\" \".join(stemmer.stem(word) for word in phrase.split()), score) for\n",
    "            #                             phrase, score in sorted_scores]\n",
    "            # print(stemmed_sorted_scores)\n",
    "            set_scores_list = []\n",
    "            for phrase, score in sorted_scores:\n",
    "                # print(phrase)\n",
    "                if phrase not in set_scores_list:\n",
    "                    set_scores_list.append((phrase,score))\n",
    "\n",
    "            pred_stemmed_phrases = set_scores_list[:5]\n",
    "            \n",
    "            source_score_list = []\n",
    "            relation_list = []\n",
    "            \"\"\"   Get Source   \"\"\"\n",
    "\n",
    "            for phrase in pred_stemmed_phrases:\n",
    "                phrase_indices = candidates_indices[phrase[0]]\n",
    "                if len(phrase_indices) == 0:\n",
    "                    continue\n",
    "                \n",
    "                phrase_source, source_score, source_sentence, source_index = get_phrase_source(candidates_indices, phrase_indices, attention_map, start_index, end_index, eos_indices)\n",
    "                source_score_list.append((phrase_source, source_score, source_sentence))\n",
    "\n",
    "                \"\"\"   Get Relation   \"\"\"\n",
    "                \n",
    "                phrase_relation = get_phrase_relation(verb_candidates_indices, phrase_indices, relation_map, start_index, end_index, source_index)\n",
    "                \n",
    "                relation_list.append(phrase_relation)\n",
    "\n",
    "                \"\"\"   END   \"\"\"\n",
    "                \n",
    "            \"\"\"   END   \"\"\"\n",
    "        \n",
    "            # layer_head_predicted_top15[(layer, head)].append(pred_stemmed_phrases)\n",
    "            sentence_predicted_top[i].append(pred_stemmed_phrases)\n",
    "            sentence_predicted_source[i].append(source_score_list)\n",
    "            sentence_predicted_relation[i].append(relation_list)\n",
    "            # for i in pred_stemmed_phrases:\n",
    "            #     if len(sentence_predicted_top[i]) == 0:\n",
    "            #         sentence_predicted_top[i] = []\n",
    "            \n",
    "            start_index = end_index\n",
    "        return sentence_predicted_top, sentence_predicted_source, sentence_predicted_relation    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ead0bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "practicadata_path = 'data/SemEval2017.jsonl'\n",
    "\n",
    "dataset = read_jsonl(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a890d9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great Wall of China, extensive bulwark erected in ancient China, one of the largest building-construction projects ever undertaken. The Great Wall actually consists of numerous walls—many of them parallel to each other—built over some two millennia across northern China and southern Mongolia. The most extensive and best-preserved version of the wall dates from the Ming dynasty (1368–1644) and runs for some 5,500 miles (8,850 km) east to west from Mount Hu near Dandong, southeastern Liaoning province, to Jiayu Pass west of Jiuquan, northwestern Gansu province. This wall often traces the crestlines of hills and mountains as it snakes across the Chinese countryside, and about one-fourth of its length consists solely of natural barriers such as rivers and mountain ridges. Nearly all of the rest (about 70 percent of the total length) is actual constructed wall, with the small remaining stretches constituting ditches or moats. Although lengthy sections of the wall are now in ruins or have disappeared completely, it is still one of the more remarkable structures on Earth. The Great Wall was designated a UNESCO World Heritage site in 1987.\n"
     ]
    }
   ],
   "source": [
    "a = {\n",
    "    'text': (\n",
    "        \"Great Wall of China, extensive bulwark erected in ancient China, one of the largest building-construction projects ever undertaken. \",\n",
    "        \"The Great Wall actually consists of numerous walls—many of them parallel to each other—built over some two millennia across northern China and southern Mongolia. \",\n",
    "        \"The most extensive and best-preserved version of the wall dates from the Ming dynasty (1368–1644) and runs for some 5,500 miles (8,850 km) east to west from Mount Hu near Dandong, southeastern Liaoning province, to Jiayu Pass west of Jiuquan, northwestern Gansu province. \",\n",
    "        \"This wall often traces the crestlines of hills and mountains as it snakes across the Chinese countryside, and about one-fourth of its length consists solely of natural barriers such as rivers and mountain ridges. \",\n",
    "        \"Nearly all of the rest (about 70 percent of the total length) is actual constructed wall, with the small remaining stretches constituting ditches or moats. \",\n",
    "        \"Although lengthy sections of the wall are now in ruins or have disappeared completely, it is still one of the more remarkable structures on Earth. \",\n",
    "        \"The Great Wall was designated a UNESCO World Heritage site in 1987.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Join into a single string\n",
    "a['text'] = ''.join(a['text'])\n",
    "\n",
    "print(a['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e632b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_by_ratio(data, min_ratio=0.5):\n",
    "    nums = data[0]  # Expecting input like [[(index, value), ...]]\n",
    "    result = [nums[0]]\n",
    "\n",
    "    for i in range(1, len(nums)):\n",
    "        current_val = nums[i][1]\n",
    "        prev_val = nums[i - 1][1]\n",
    "\n",
    "        if current_val == 0:\n",
    "            break\n",
    "        if current_val < min_ratio * prev_val:\n",
    "            break\n",
    "\n",
    "        result.append(nums[i])\n",
    "\n",
    "    return [result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22b9e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table_label(sentence_label, phrase, attention_str):\n",
    "    return f'''<\n",
    "        <TABLE BORDER=\"0\" CELLBORDER=\"0\" CELLSPACING=\"0\">\n",
    "            <TR><TD><FONT COLOR=\"red\" POINT-SIZE=\"8\">{sentence_label}:</FONT></TD></TR>\n",
    "            <TR><TD><FONT COLOR=\"blue\" POINT-SIZE=\"16\">{phrase[0]}</FONT></TD></TR>\n",
    "            <TR><TD><FONT COLOR=\"red\" POINT-SIZE=\"6\">{attention_str}</FONT></TD></TR>\n",
    "        </TABLE>\n",
    "    >'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "398fdc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "[[('.', 'LESS')], [('great', 'NNP'), ('wall', 'NNP'), ('of', 'LESS'), ('china', 'NNP'), (',', 'LESS'), ('extensive', 'JJ'), ('bulwark', 'NN'), ('erected', 'VBN'), ('in', 'LESS'), ('ancient', 'JJ'), ('china', 'NNP'), (',', 'LESS'), ('one', 'CD'), ('of', 'LESS'), ('the', 'DT'), ('largest', 'JJS'), ('building-construction', 'JJ'), ('projects', 'NNS'), ('ever', 'RB'), ('undertaken', 'VBN'), ('.', 'LESS')], [('the', 'DT'), ('great', 'NNP'), ('wall', 'NNP'), ('actually', 'RB'), ('consists', 'VBZ'), ('of', 'LESS'), ('numerous', 'JJ'), ('walls', 'NNS'), ('--', 'LESS'), ('many', 'JJ'), ('of', 'LESS'), ('them', 'PRP'), ('parallel', 'VBP'), ('to', 'LESS'), ('each', 'DT'), ('other', 'JJ'), ('--', 'LESS'), ('built', 'VBN'), ('over', 'IN'), ('some', 'DT'), ('two', 'CD'), ('millennia', 'NNS'), ('across', 'IN'), ('northern', 'JJ'), ('china', 'NNP'), ('and', 'CC'), ('southern', 'JJ'), ('mongolia', 'NNP'), ('.', 'LESS')], [('the', 'DT'), ('most', 'RBS'), ('extensive', 'JJ'), ('and', 'CC'), ('best-preserved', 'JJ'), ('version', 'NN'), ('of', 'LESS'), ('the', 'DT'), ('wall', 'NN'), ('dates', 'NNS'), ('from', 'IN'), ('the', 'DT'), ('ming', 'NNP'), ('dynasty', 'NN'), ('-lrb-', '-LRB-'), ('1368', 'CD'), ('--', 'LESS'), ('1644', 'CD'), ('-rrb-', '-RRB-'), ('and', 'CC'), ('runs', 'VBZ'), ('for', 'IN'), ('some', 'DT'), ('5,500', 'CD'), ('miles', 'NNS'), ('-lrb-', '-LRB-'), ('8,850', 'CD'), ('km', 'LESS'), ('-rrb-', '-RRB-'), ('east', 'JJ'), ('to', 'LESS'), ('west', 'NN'), ('from', 'IN'), ('mount', 'NNP'), ('hu', 'LESS'), ('near', 'IN'), ('dandong', 'NNP'), (',', 'LESS'), ('southeastern', 'JJ'), ('liaoning', 'NNP'), ('province', 'NN'), (',', 'LESS'), ('to', 'LESS'), ('jiayu', 'NNP'), ('pass', 'VB'), ('west', 'NN'), ('of', 'LESS'), ('jiuquan', 'NNP'), (',', 'LESS'), ('northwestern', 'JJ'), ('gansu', 'NNP'), ('province', 'NN'), ('.', 'LESS')], [('this', 'DT'), ('wall', 'NN'), ('often', 'RB'), ('traces', 'NNS'), ('the', 'DT'), ('crestlines', 'NNS'), ('of', 'LESS'), ('hills', 'NNS'), ('and', 'CC'), ('mountains', 'NNS'), ('as', 'LESS'), ('it', 'LESS'), ('snakes', 'VBZ'), ('across', 'IN'), ('the', 'DT'), ('chinese', 'JJ'), ('countryside', 'NN'), (',', 'LESS'), ('and', 'CC'), ('about', 'IN'), ('one-fourth', 'NN'), ('of', 'LESS'), ('its', 'PRP$'), ('length', 'NN'), ('consists', 'VBZ'), ('solely', 'RB'), ('of', 'LESS'), ('natural', 'JJ'), ('barriers', 'NNS'), ('such', 'JJ'), ('as', 'LESS'), ('rivers', 'NNS'), ('and', 'CC'), ('mountain', 'NN'), ('ridges', 'NNS'), ('.', 'LESS')], [('nearly', 'RB'), ('all', 'DT'), ('of', 'LESS'), ('the', 'DT'), ('rest', 'NN'), ('-lrb-', '-LRB-'), ('about', 'IN'), ('70', 'LESS'), ('percent', 'NN'), ('of', 'LESS'), ('the', 'DT'), ('total', 'JJ'), ('length', 'NN'), ('-rrb-', '-RRB-'), ('is', 'LESS'), ('actual', 'JJ'), ('constructed', 'VBN'), ('wall', 'NN'), (',', 'LESS'), ('with', 'IN'), ('the', 'DT'), ('small', 'JJ'), ('remaining', 'VBG'), ('stretches', 'NNS'), ('constituting', 'VBG'), ('ditches', 'NNS'), ('or', 'LESS'), ('moats', 'NNS'), ('.', 'LESS')], [('although', 'IN'), ('lengthy', 'JJ'), ('sections', 'NNS'), ('of', 'LESS'), ('the', 'DT'), ('wall', 'NN'), ('are', 'VBP'), ('now', 'RB'), ('in', 'LESS'), ('ruins', 'NNS'), ('or', 'LESS'), ('have', 'VBP'), ('disappeared', 'VBN'), ('completely', 'RB'), (',', 'LESS'), ('it', 'LESS'), ('is', 'LESS'), ('still', 'RB'), ('one', 'CD'), ('of', 'LESS'), ('the', 'DT'), ('more', 'RBR'), ('remarkable', 'JJ'), ('structures', 'NNS'), ('on', 'LESS'), ('earth', 'NNP'), ('.', 'LESS')], [('the', 'DT'), ('great', 'NNP'), ('wall', 'NNP'), ('was', 'VBD'), ('designated', 'VBN'), ('a', 'LESS'), ('unesco', 'NNP'), ('world', 'NNP'), ('heritage', 'NNP'), ('site', 'NN'), ('in', 'LESS'), ('1987', 'CD'), ('.', 'LESS')]]\n",
      "['although', 'runs for', 'on', 'in', '.', 'across', 'have disappeared', 'other --', 'extensive and', 'consists of', 'of', 'consists', 'it', 'undertaken .', 'designated a', 'about', 'erected in', 'parallel to', 'and', 'is', 'or', 'snakes across', 'constituting', ',', 'such as', '70', 'remaining', 'was', 'are', 'from', 'to', 'built over', 'pass', 'constructed', '--', 'hu', 'many of', 'near', 'km', 'east to', 'as', 'with']\n"
     ]
    }
   ],
   "source": [
    "predicts, sources, relations = get_flow(args, a, model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca70ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.pdf'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkpoint\n",
    "dot = Digraph()\n",
    "\n",
    "existing_nodes = set()\n",
    "phrase_sentences = defaultdict(set)\n",
    "phrase_attention = defaultdict(set)\n",
    "dot.attr(rankdir='TB', size='8,5')\n",
    "\n",
    "threshold = 0.5\n",
    "print(len(predicts))\n",
    "# Create nodes and edges\n",
    "for i in range(len(predicts)):\n",
    "    p = predicts[i]\n",
    "    s = sources[i]\n",
    "    r = relations[i]\n",
    "\n",
    "    if len(p) == 0:\n",
    "        continue\n",
    "\n",
    "    p = trim_by_ratio(p,0.25) \n",
    "    s = s[:len(p[0])]\n",
    "    r = r[:len(p[0])]\n",
    "\n",
    "    with dot.subgraph() as sgraph:\n",
    "    #     sgraph.attr(rank='same')\n",
    "        for phrase, source, relation in zip(p[0], s[0], r[0]):\n",
    "            \n",
    "            if phrase[1] > threshold:\n",
    "\n",
    "                phrase_sentences[phrase[0]].add(i + 1)\n",
    "                sentence_str = ', '.join(map(str, sorted(phrase_sentences[phrase[0]])))\n",
    "                phrase_attention[phrase[0]].add(round(float(phrase[1]),3))\n",
    "                attention_str = ', '.join(map(str, sorted(phrase_attention[phrase[0]])))\n",
    "                sentence_label = f'sentence {sentence_str}'\n",
    "\n",
    "                if phrase[0] not in existing_nodes:\n",
    "                    existing_nodes.add(phrase[0])\n",
    "                    dot.node(phrase[0], make_table_label(sentence_label, phrase, attention_str), shape='box')\n",
    "\n",
    "                    if source[0]:\n",
    "                        phrase_sentences[source[0]].add(source[2])\n",
    "                        sentence_str = ', '.join(map(str, sorted(phrase_sentences[source[0]])))\n",
    "                        phrase_attention[source[0]].add(round(float(source[1]),3))\n",
    "                        attention_str = ', '.join(map(str, sorted(phrase_attention[source[0]])))\n",
    "                        sentence_label = f'sentence {sentence_str}'\n",
    "                        \n",
    "                        if source[0] not in existing_nodes:\n",
    "                            existing_nodes.add(source[0])\n",
    "                        \n",
    "                        dot.node(source[0], make_table_label(sentence_label, source, attention_str), shape='box')\n",
    "                        dot.edge(source[0], phrase[0], label=relation if source[2]== i else '')\n",
    "                # else:\n",
    "                #     dot.node(phrase[0], make_table_label(sentence_label, phrase, attention_str), shape='box')\n",
    "\n",
    "    # with dot.subgraph() as sgraph:\n",
    "    #     sgraph.attr(rank='same')\n",
    "    #     for phrase, _ in p[0]:\n",
    "    #         if phrase not in existing_nodes:\n",
    "    #             existing_nodes.add(phrase)\n",
    "    #             dot.node(phrase, phrase)\n",
    "\n",
    "\n",
    "dot.render(view=True)  # Or just display in notebook: dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3be26374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are 'Dynasty' and 'ruins' connected? False\n",
      "Are 'Ming' and 'structures' connected? False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English model\n",
    "nlp2 = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = a['text']\n",
    "\n",
    "doc = nlp2(text)\n",
    "\n",
    "# # Print dependency parse info for tokens\n",
    "# for token in doc:\n",
    "#     print(token.text, token.dep_, token.head.text)\n",
    "\n",
    "# Function to check if two tokens are connected syntactically\n",
    "def are_connected(token1, token2):\n",
    "    # Check if token1 is ancestor of token2 or vice versa\n",
    "    return token1 in token2.ancestors or token2 in token1.ancestors\n",
    "\n",
    "# Find tokens for 'Ming' and 'ruins'\n",
    "token_ming = [t for t in doc if t.text == \"wall\"][0]\n",
    "token_ruins = [t for t in doc if t.text == \"length\"][0]\n",
    "token_structures = [t for t in doc if t.text == \"structures\"][0]\n",
    "\n",
    "print(\"Are 'Dynasty' and 'ruins' connected?\", are_connected(token_ming, token_ruins))  # likely False\n",
    "print(\"Are 'Ming' and 'structures' connected?\", are_connected(token_ming, token_structures))  # likely True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0d4b6b",
   "metadata": {},
   "source": [
    "# MAKING TREE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keyphrase_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
